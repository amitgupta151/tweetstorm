{
 "metadata": {
  "name": "",
  "signature": "sha256:947ee75ee63996033f744eb4b599b7758717ac504725f5302506b64b36551506"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import psycopg2\n",
      "import csv, sys\n",
      "from collections import defaultdict\n",
      "import string, re, operator, json\n",
      "from datetime import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "conn = psycopg2.connect(\"host=localhost dbname=cooccurrence user=vagrant password=vagrant\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# cur = conn.cursor()\n",
      "# cur.execute(\"SELECT * FROM documents_document LIMIT 10\")\n",
      "# cur.fetchone()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Load articles to work with \n",
      "path_to_store = \"/vagrant/julia/VerbalPhraseDetection/tweetstorm/features/utils/work/\"\n",
      "# Read web articles\n",
      "f = open('/vagrant/julia/VerbalPhraseDetection/popularity/data/web1.csv', 'r')\n",
      "web = {}\n",
      "for el in f.readlines():\n",
      "    splitted = el.split('_____@@@@@_____')\n",
      "    if len(splitted) > 4:\n",
      "        web[splitted[0]] = splitted[1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Read tweets articles\n",
      "# f = open('/vagrant/julia/VerbalPhraseDetection/popularity/data/tweets.csv', 'r')\n",
      "# tweets = {}\n",
      "# i = 0\n",
      "# for el in f:\n",
      "#     if i > 100:\n",
      "#         break\n",
      "#     splitted = el.split('_____@@@@@_____')\n",
      "#     if len(splitted) > 4:\n",
      "#         tweets[splitted[0]] = splitted[1:-1]\n",
      "#     i += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# datetime.strptime(web.items()[0][1][1].split('.')[0], \"%Y-%m-%d %H:%M:%S\").isoformat(\" \")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Get concepts/literals to scan the docs\n",
      "concept_literals = []\n",
      "concept_literals_map = defaultdict(list)\n",
      "f = open('/vagrant/julia/VerbalPhraseDetection/popularity/data/theme_climate_5k_plus_clean_non-clean.json', 'r')\n",
      "j = json.loads(f.read())\n",
      "for el in j:\n",
      "    try:\n",
      "        concept_literals.append((el['main'].decode('utf-8').encode('ascii','ignore'), el['id'], 1.))\n",
      "        concept_literals_map[el['id']].append(el['main'].decode('utf-8').encode('ascii','ignore'))\n",
      "    except Exception, e:\n",
      "        continue\n",
      "    for lit in el['literals']:\n",
      "        try:\n",
      "            concept_literals.append((lit.decode('utf-8').encode('ascii','ignore'), el['id'], 1.))\n",
      "            concept_literals_map[el['id']].append(lit.decode('utf-8').encode('ascii','ignore'))\n",
      "        except Exception, e:\n",
      "            continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Insert into the DB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sys.path.append('/vagrant/julia/datalib/')\n",
      "import datalib\n",
      "\n",
      "# Initialize the tokenizer with the literals\n",
      "datalib.tokenizers.AbbrAwareStemmer(datalib.tokenizers.SnowballStemmer('english'))\n",
      "token = datalib.VocabularyTokenizer\n",
      "tokenizer = token.build(\n",
      "    concept_literals, [],                                                                          \n",
      "    datalib.tokenizers.AbbrAwareStemmer(datalib.tokenizers.SnowballStemmer('english'))\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def add_cooccurrences(concepts, doc_id, cur):\n",
      "    for con1 in concepts[:-1]:\n",
      "        for con2 in concepts[1:]:\n",
      "            if con1[3] < con2[3]:\n",
      "                if concept_literals_map[con1[1]][0] < concept_literals_map[con2[1]][0]:\n",
      "                    cur.execute('INSERT INTO cooccurrences_sentence VALUES (DEFAULT, %s, %s, NULL);',\n",
      "                        (con1[0], con2[0]))\n",
      "                else:\n",
      "                    cur.execute('INSERT INTO cooccurrences_sentence VALUES (DEFAULT, %s, %s, NULL);',\n",
      "                        (con2[0], con1[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "cur.close()\n",
      "conn.rollback()\n",
      "for k, v in web.items():\n",
      "    cur = conn.cursor()\n",
      "    doc_id = k\n",
      "    identifier = v[0]\n",
      "    publish_date = datetime.strptime(v[1].split('.')[0].split('+')[0], \"%Y-%m-%d %H:%M:%S\")\n",
      "    title = v[2]\n",
      "    text = v[3]\n",
      "    cur.execute(\"INSERT INTO documents_document(id, published_date, title, extracted_text, identifier) VALUES(%s, %s, %s, %s, %s);\", \n",
      "                (doc_id, publish_date, title, text, identifier))\n",
      "    conn.commit()\n",
      "    cur.close()\n",
      "print \"Done!\"\n",
      "\n",
      "conn.rollback()\n",
      "for k, v in concept_literals_map.items()[10:]:\n",
      "    cur = conn.cursor()\n",
      "    concept_id = k\n",
      "    main_name = v[0]\n",
      "    cur.execute('INSERT INTO concepts_concept(id, name) VALUES (%s, %s)',\n",
      "                (concept_id, main_name))\n",
      "    cur.execute('INSERT INTO concepts_literal VALUES (DEFAULT, NULL, %s, %s, %s);',\n",
      "                (main_name, concept_id, True))\n",
      "    for s in v[1:]:\n",
      "        cur.execute('INSERT INTO concepts_literal VALUES (DEFAULT, NULL, %s, %s, %s);',\n",
      "                    (s, concept_id, False))\n",
      "    conn.commit()\n",
      "    cur.close()\n",
      "print \"Done!\"\n",
      "\n",
      "conn.rollback()\n",
      "# occurrence_info = defaultdict(list)\n",
      "for doc_id, doc in web.items():\n",
      "    cur = conn.cursor()\n",
      "    text = doc[2].decode('utf-8').encode('ascii','ignore') + '\\n' + doc[3].decode('utf-8').encode('ascii','ignore')\n",
      "    data = tokenizer.analyze((text))\n",
      "    for di in data:\n",
      "        for key, value in di.items():\n",
      "            cur.execute('INSERT INTO concepts_occurrence VALUES (DEFAULT, %s, %s, %s, %s);',\n",
      "                (key.id, doc_id, value.positions[0][0], value.positions[-1][0] - value.positions[0][0] + value.positions[-1][1]))\n",
      "    conn.commit()\n",
      "    cur.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done!\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "conn.rollback()\n",
      "# occurrence_info = defaultdict(list)\n",
      "# j = 10000\n",
      "for doc_id, doc in web.items():\n",
      "#     if j % 1000 == 0:\n",
      "#         print j\n",
      "    text = doc[2].decode('utf-8').encode('ascii','ignore') + '\\n' + doc[3].decode('utf-8').encode('ascii','ignore')\n",
      "    cur = conn.cursor()\n",
      "    cur.execute('SELECT * FROM concepts_occurrence WHERE document_id=%s;', (doc_id,) )\n",
      "    occurrence_info = cur.fetchall()\n",
      "    occurrence_info = sorted(occurrence_info, key=lambda x:x[3])\n",
      "    current_concept = 0\n",
      "    for i, el in enumerate(occurrence_info[:-1]):\n",
      "        # Add terms to cooccurrence if in the same sentence\n",
      "        # If there is a dot between cur and next and there are more then 1 word\n",
      "        # print \".............\"\n",
      "        # print occurrence_info[doc[0]][current_concept][1][0][0], occurrence_info[doc[0]][i+1][1][0][0]\n",
      "        # print text[occurrence_info[doc[0]][current_concept][1][0][0]:occurrence_info[doc[0]][i+1][1][0][0] + 20]\n",
      "        # print re.findall(\"\\S*[^\\w\\s]\\S*\", text[occurrence_info[doc[0]][current_concept][1][0][0]:occurrence_info[doc[0]][i+1][1][0][0]])\n",
      "        if el[3] < occurrence_info[i+1][3] and \\\n",
      "            re.findall(\"\\S*[^\\w\\s\\-]\\S*\", text[occurrence_info[current_concept][3]:occurrence_info[i+1][3]]):\n",
      "            if current_concept + 1 < i:\n",
      "                add_cooccurrences(occurrence_info[current_concept:i], doc_id, cur)\n",
      "            current_concept = i + 1\n",
      "    conn.commit()\n",
      "    cur.close()\n",
      "#     j += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'int' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-30-181516f87a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Get Verbal Phrases for Top"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import sys, json\n",
      "import getopt, codecs, time, pickle\n",
      "from collections import Counter\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "from outputVerbalPhrase import * \n",
      "from concept_occurrence import *\n",
      "from get_verbal_phrase import *\n",
      "from get_parse_tree import parse_fileTextBlob\n",
      "sys.path.append(\".\")\n",
      "sys.path.append(\"/opt/texpp\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_punkt(text, con_pos, where):\n",
      "    p = re.compile(\"[^\\w\\s\\-]\")\n",
      "    if where == 'left':\n",
      "        position = 0\n",
      "        for m in p.finditer(text[0: con_pos]):\n",
      "            position = m.start()\n",
      "    else:\n",
      "        position = con_pos\n",
      "        for m in p.finditer(text[con_pos:]):\n",
      "            position = m.start()\n",
      "            break\n",
      "    return position\n",
      "\n",
      "def find_sentence(text, con1_pos, con2_pos):\n",
      "    if con1_pos < con2_pos:\n",
      "        return find_punkt(text, con1_pos, 'left')+1,con2_pos + find_punkt(text, con2_pos, 'right')\n",
      "    else:\n",
      "        return find_punkt(text, con2_pos, 'left')+1,con1_pos + find_punkt(text, con1_pos, 'right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parse_trees_path = '/vagrant/julia/VerbalPhraseDetection/tweetstorm/data/parse_trees/'\n",
      "parser_path = '/vagrant/stanford-parser-2012-11-12/lexparser.sh'\n",
      "labels_map = json.loads(open('/vagrant/julia/VerbalPhraseDetection/tweetstorm/data/concepts_with_synonyms.json', 'r').read())\n",
      "smart = 2\n",
      "# Get most coocccurred - 1st for loop\n",
      "cur = conn.cursor()\n",
      "cur.execute('SELECT * FROM cooccurrences_sentence LIMIT 1;')\n",
      "cooccurrence_info = cur.fetchone()\n",
      "conn.commit()\n",
      "cur.close()\n",
      "print cooccurrence_info\n",
      "# For docs for the most cooccurred - 2nd for loop\n",
      "    # Get cooccurrence for documents\n",
      "parse_tree = \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(21309, 3462931, 3462933, None)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get concept id of the most cooccurred\n",
      "cur = conn.cursor()\n",
      "cur.execute('select c1.concept_id, c2.concept_id, count(*) from cooccurrences_sentence \\\n",
      "join concepts_occurrence as c1 on cooccurrences_sentence.concept_occurrence_id_1 = c1.id \\\n",
      "join concepts_occurrence as c2 on cooccurrences_sentence.concept_occurrence_id_2 = c2.id \\\n",
      "group by c1.concept_id, c2.concept_id \\\n",
      "order by count(*) DESC \\\n",
      "limit 30;')\n",
      "cons = cur.fetchall()\n",
      "conn.commit()\n",
      "cur.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Get cooccurrences for the inserted document with id=1\n",
      "# cur = conn.cursor()\n",
      "# cur.execute('select c1.concept_id, c2.concept_id, count(*) \\\n",
      "# from cooccurrences_sentence join concepts_occurrence as c1 \\\n",
      "# on cooccurrences_sentence.concept_occurrence_id_1 = c1.id \\\n",
      "# join concepts_occurrence as c2 \\\n",
      "# on cooccurrences_sentence.concept_occurrence_id_2 = c2.id \\\n",
      "# where c1.document_id=1 \\\n",
      "# group by c1.concept_id, c2.concept_id \\\n",
      "# order by count(*) DESC \\\n",
      "# limit 30;')\n",
      "# cons = cur.fetchall()\n",
      "# conn.commit()\n",
      "# cur.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import wordnet as wn\n",
      "def fill_union_find_for_synset(verb, unique_verbs, union_find, debug=0):\n",
      "    # check other if it is in synset of mine\n",
      "    for k, v in unique_verbs.items():\n",
      "        if k != verb:\n",
      "            if k in unique_verbs[verb]['synset']:\n",
      "                if debug:\n",
      "                    print \"Verb - %s - is in synset of %s\" % (k, verb)\n",
      "                union_find[v['counter']] = unique_verbs[verb]['counter']\n",
      "    # check if this verbs in synset of other\n",
      "    for k, v in unique_verbs.items():\n",
      "        if verb != k:\n",
      "            if verb in v['synset']:\n",
      "                union_find[unique_verbs[verb]['counter']] = v['counter']\n",
      "                if debug:\n",
      "                    print \"Verb - %s - is in synset of %s\" % (verb, k)\n",
      "                return [(kk, vv) for kk, vv in unique_verbs.items() if vv['counter']==v['counter']][0]\n",
      "    return verb, unique_verbs[verb]\n",
      "\n",
      "def get_folded_statistics(cleaned_triplets, debug=0):\n",
      "    count_unique = 0\n",
      "    unique_verbs = defaultdict(dict)\n",
      "    union_find_for_cleaned = [i for i in xrange(len(cleaned_triplets))]\n",
      "    statistics_cleaned = {'->': defaultdict(int), '<-': defaultdict(int)}\n",
      "    for tr in cleaned_triplets:\n",
      "        res = \"\"\n",
      "        if debug:\n",
      "            print \"Initial:\", tr.final_verbal_phrase\n",
      "        if 'to ' in tr.final_verbal_phrase:\n",
      "            verb_to_add = tr.final_verbal_phrase.split('to ')[1]\n",
      "        else:\n",
      "            verb_to_add = tr.final_verbal_phrase\n",
      "        if debug:\n",
      "            print \"Deleted to: \", verb_to_add\n",
      "        if ' and ' in verb_to_add:\n",
      "            verb_to_add = verb_to_add.split(' and ')[0]\n",
      "        if debug:\n",
      "            print \"Deleted and: \", verb_to_add\n",
      "        words = nltk.pos_tag(verb_to_add.split(' '))\n",
      "        if debug:\n",
      "            print \"POS: \", words\n",
      "        while len(words) > 0 and \\\n",
      "            not words[0][1].startswith('N') and \\\n",
      "            not words[0][1].startswith('V') and \\\n",
      "            not words[0][1].startswith('J'): #  ('IN' and 'RB'):\n",
      "            if debug:\n",
      "                print \"Tag for the word: \", words[0][1], words[0][1].startswith('N'), words[0][1].startswith('V')\n",
      "            if words[0][1] == 'RB':\n",
      "                res = words[0][0] + ' '\n",
      "            words.pop(0)\n",
      "        if debug:\n",
      "            print \"Verbal phases after deleting start with bad POS: \", words\n",
      "        if words:\n",
      "            res += words[0][0]\n",
      "            # Synset folding\n",
      "            if words[0][0] not in unique_verbs:\n",
      "                unique_verbs[words[0][0]]['counter'] = count_unique\n",
      "                count_unique += 1\n",
      "                unique_verbs[words[0][0]]['synset'] = \\\n",
      "                    dict(sorted(Counter([re.sub(\"_\", \" \", x.name()) \n",
      "                         for bla in [x.lemmas() \n",
      "                                     for x in wn.synsets(words[0][0], pos=wn.VERB)] \n",
      "                         for x in bla]).items(), key=lambda x:x[1], reverse=True)[0:5]).keys()\n",
      "                if debug:\n",
      "                    print 'Synsets and counter for the verb: ', words[0][0], unique_verbs[words[0][0]]\n",
      "                parent_verb = fill_union_find_for_synset(words[0][0], unique_verbs, union_find_for_cleaned, debug)\n",
      "                if debug:\n",
      "                    print \"Parent verb - %s, for verb - %s\" % (parent_verb[0], words[0][0])\n",
      "            if debug:\n",
      "                print \"First word: \", res\n",
      "            if len(words) == 1 and words[0][1] == 'DT':\n",
      "                continue\n",
      "            if len(words) > 1 and words[1][1] == 'IN':\n",
      "                res += ' ' + words[1][0]\n",
      "                if debug:\n",
      "                    print \"Final phrase: \", res\n",
      "            elif len(words) > 2 and words[2][1] == 'IN':\n",
      "                res += ' ' + words[1][0] + ' ' + words[2][0]\n",
      "                if debug:\n",
      "                    print \"Final phrase: \", res\n",
      "            if tr.subject == 0:\n",
      "                statistics_cleaned['->'][res] += 1\n",
      "                if not 'representations->' in unique_verbs[words[0][0]]:\n",
      "                    unique_verbs[words[0][0]]['representations->']  = [res]\n",
      "                else:\n",
      "                    unique_verbs[words[0][0]]['representations->'].append(res)\n",
      "            else:\n",
      "                statistics_cleaned['<-'][res] += 1\n",
      "                if not 'representations<-' in unique_verbs[words[0][0]]:\n",
      "                    unique_verbs[words[0][0]]['representations<-']  = [res]\n",
      "                else:\n",
      "                    unique_verbs[words[0][0]]['representations<-'].append(res)\n",
      "#     print \"[CLEANED]\"\n",
      "    print \"->\"\n",
      "    res_right = []\n",
      "    for kk, vv in sorted([(k, v) for k, v in statistics_cleaned['->'].items()], key=lambda x:x[1], reverse=True)[0:5]:\n",
      "        if kk.split()[0] not in res_right:\n",
      "            try:\n",
      "                print kk, len(unique_verbs[kk.split()[0]]['representations->'])\n",
      "            except:\n",
      "                continue\n",
      "            res_right.append(kk.split()[0])\n",
      "    print \"<-\"\n",
      "    res_left = []\n",
      "    for kk, vv in sorted([(k, v) for k, v in statistics_cleaned['<-'].items()], key=lambda x:x[1], reverse=True)[0:5]:\n",
      "        if kk.split()[0] not in res_left:\n",
      "            try:\n",
      "                print kk, len(unique_verbs[kk.split()[0]]['representations<-'])\n",
      "            except:\n",
      "                continue\n",
      "            res_left.append(kk.split()[0])\n",
      "    return sorted([(k, v) for k, v in statistics_cleaned['->'].items()], key=lambda x:x[1], reverse=True), sorted([(k, v) for k, v in statistics_cleaned['<-'].items()], key=lambda x:x[1], reverse=True), unique_verbs, union_find_for_cleaned"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result_to_rank = []\n",
      "import nltk\n",
      "file_type = 'news'\n",
      "num_pages = (0,500)\n",
      "for con in cons:\n",
      "    cur = conn.cursor()\n",
      "    cur.execute('select c1.id, c1.concept_id, c1.position, c1.length, c2.id, c2.concept_id, c2.position, c2.length, c1.document_id from cooccurrences_sentence \\\n",
      "    join concepts_occurrence as c1 on cooccurrences_sentence.concept_occurrence_id_1 = c1.id \\\n",
      "    join concepts_occurrence as c2 on cooccurrences_sentence.concept_occurrence_id_2 = c2.id \\\n",
      "    where c1.concept_id = %s and c2.concept_id = %s;', (con[0], con[1],) )\n",
      "    occurrences = cur.fetchall()\n",
      "    conn.commit()\n",
      "    cur.close()\n",
      "    id_parse_trees = {}\n",
      "    count_of_being_one_concept = 0\n",
      "    output_name_pickle = 'work/ipython_issue_discovery/%s-statistics-%s-%s-%d-%d.pickle' % (file_type, con[0], con[1], num_pages[0], num_pages[1])\n",
      "    cname1 = \"\"\n",
      "    cname2 = \"\"\n",
      "    if os.path.exists(output_name_pickle): # and file_type == 'twitter':\n",
      "        triplets = pickle.loads( open(output_name_pickle, 'r').read() )\n",
      "    else:\n",
      "        for occs in occurrences[0:500]:\n",
      "            cur = conn.cursor()\n",
      "            cur.execute('SELECT * FROM concepts_occurrence WHERE id=%s;', (occs[0],) )\n",
      "            occurrence1 = cur.fetchone()\n",
      "            cur.execute('SELECT * FROM concepts_concept WHERE id=%s;', (occs[1],) )\n",
      "            con1 = cur.fetchone()\n",
      "            cur.execute('SELECT * FROM concepts_occurrence WHERE id=%s;', (occs[4],) )\n",
      "            occurrence2 = cur.fetchone()\n",
      "            if occurrence2[3] > occurrence1[3] and 0 < occurrence2[3] - occurrence1[3] - occurrence1[4] < 2:\n",
      "                count_of_being_one_concept += 1\n",
      "                continue\n",
      "            if occurrence2[3] < occurrence1[3] and 0 < occurrence1[3] - occurrence2[3] - occurrence2[4] < 2:\n",
      "                count_of_being_one_concept += 1\n",
      "                continue\n",
      "            cur.execute('SELECT * FROM concepts_concept WHERE id=%s;', (occs[5],) )\n",
      "            con2 = cur.fetchone()\n",
      "            cur.execute('SELECT * FROM documents_document WHERE id=%s;', (occs[8],) )\n",
      "            document = cur.fetchone()\n",
      "            doc_id = document[0]\n",
      "            doc_text = document[2].decode('utf-8').encode('ascii','ignore') + '\\n' + document[6].decode('utf-8').encode('ascii','ignore')\n",
      "            conn.commit()\n",
      "            cur.close()\n",
      "\n",
      "            start, end = find_sentence(doc_text, occurrence1[3], occurrence2[3]+occurrence2[4])\n",
      "            text = doc_text[start:end]\n",
      "#             print \"TEXT:\", text\n",
      "\n",
      "            id_to_store = str(doc_id) + '_' + str(start) + '_' + str(end)\n",
      "            parse_tree_info = parse_fileTextBlob(parse_trees_path + id_to_store, text, parser_path, str(id_to_store), smart, {}, None)\n",
      "            id_parse_trees[parse_tree_info[0]] = parse_tree_info[1]\n",
      "\n",
      "        triplets = parse_triplets(id_parse_trees=id_parse_trees, \n",
      "                              labels_map=labels_map, \n",
      "                              concepts_to_find=[con1[2],con2[2]], \n",
      "                              parser_path=parser_path, debug=0)\n",
      "#         open(output_name_pickle, 'w'). write( pickle.dumps(triplets) )\n",
      "    cur = conn.cursor()\n",
      "    cur.execute('SELECT * FROM concepts_concept WHERE id=%s;', (occurrences[0][1],) )\n",
      "    con1 = cur.fetchone()\n",
      "    cname1 = con1[2]\n",
      "    cur.execute('SELECT * FROM concepts_concept WHERE id=%s;', (occurrences[0][5],) )\n",
      "    con2 = cur.fetchone()\n",
      "    cname2 = con2[2]\n",
      "    print '\\n', cname1, 'VS', cname2\n",
      "    print count_of_being_one_concept / 500.0\n",
      "    cleaned_triplets = clean_triplets([tr[0] for tr in triplets])\n",
      "    statistics = get_statistics([tr[0] for tr in triplets])\n",
      "#     print \"[STATISTICS]\"\n",
      "#     print statistics\n",
      "    f, b, nn, mm = get_folded_statistics(cleaned_triplets)\n",
      "    r_t_r = (' '.join([cname1, 'VS', cname2]), \n",
      "             count_of_being_one_concept / 500.0, \n",
      "             f, b)\n",
      "    result_to_rank.append(r_t_r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Carbon VS Pollution\n",
        "0.0\n",
        "->\n",
        "curb by 1\n",
        "make 1\n",
        "taxchanges 1\n",
        "<-\n",
        "best measure by 2\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "US Environmental Protection Agency VS regulation\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "issue 9\n",
        "propose 11\n",
        "determine that 3\n",
        "<-\n",
        "feel about 1\n",
        "come from 1\n",
        "ever issue by impose by 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS Power Plant\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "fire 11\n",
        "burn 2\n",
        "describe 2\n",
        "cool 2\n",
        "be 1\n",
        "<-\n",
        "use than 4\n",
        "begin bring in 1\n",
        "burn 1\n",
        "switch from 1\n",
        "produce 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Oil VS natural gas\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "produce 2\n",
        "typically combine with replace as 1\n",
        "typically situate in rise while 1\n",
        "<-\n",
        "replace in 3\n",
        "limit 2\n",
        "decline 1\n",
        "Is 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS natural gas\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "use 3\n",
        "extraction from 2\n",
        "transport than 2\n",
        "plantsPulverized with 1\n",
        "fill in 1\n",
        "<-\n",
        "displace 7\n",
        "idle here in 3\n",
        "surpass as 3\n",
        "decrease 2\n",
        "take over 3\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Drilling VS Oil\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extract from 5\n",
        "drill in 3\n",
        "collect 2\n",
        "<-\n",
        "announce that 4\n",
        "leave 1\n",
        "navigate through 1\n",
        "typically require begin as 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fossil VS Fuel\n",
        "0.0\n",
        "->\n",
        "<-\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pollution VS Water\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "find in 2\n",
        "Were pump endanger 1\n",
        "get into 1\n",
        "cloud 1\n",
        "<-\n",
        "detect 3\n",
        "address by 2\n",
        "deplete of 1\n",
        "set for 1\n",
        "necessarily mean from "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Climate change VS War\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "indirectly increase by directly cause grow 3\n",
        "were 2\n",
        "exacerbate in 1\n",
        "<-\n",
        "emerge as 1\n",
        "interlink with 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Carbon VS US Environmental Protection Agency\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "remember that 1\n",
        "axe in 1\n",
        "oppose 1\n",
        "offer 1\n",
        "<-\n",
        "propose on 34\n",
        "limit from 17\n",
        "cut from 13\n",
        "issue 8\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Drilling VS shale\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "head on 63\n",
        "generally require underground under 2\n",
        "perfect in 2\n",
        "penetrate pumped down 2\n",
        "<-\n",
        "bring on 4\n",
        "pay 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Climate change VS effects of climate\n",
        "0.0\n",
        "->\n",
        "need from 1\n",
        "<-\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Carbon VS Prices\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "represent 2\n",
        "dont 2\n",
        "store by 1\n",
        "make sure that 1\n",
        "destroy that 1\n",
        "<-\n",
        "represent 2\n",
        "set for 2\n",
        "secure from 2\n",
        "reflect 2\n",
        "signalThats 2\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Flood VS Water\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "share point out 2\n",
        "By 2\n",
        "result in 1\n",
        "ineffective at 1\n",
        "calculate base on 1\n",
        "<-\n",
        "tackle in 1\n",
        "face from 1\n",
        "percolate well for 1\n",
        "mean that 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Climate change VS Risk\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "increase 21\n",
        "pose 11\n",
        "put at 5\n",
        "endanger by 3\n",
        "result in 2\n",
        "<-\n",
        "adapt 3\n",
        "low if 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS Pollution\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "gasify 2\n",
        "burn 2\n",
        "completely eliminate stateand show that 2\n",
        "emit 2\n",
        "<-\n",
        "associate with 3\n",
        "result from 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pollution VS US Environmental Protection Agency\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "ensure that 1\n",
        "announce by 1\n",
        "offer 1\n",
        "<-\n",
        "release in 22\n",
        "limit from 18\n",
        "propose 18\n",
        "issue 14\n",
        "cut from 10\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Drilling VS fracking\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "generally require utilize 2\n",
        "effectively banned by access 1\n",
        "contaminate 1\n",
        "<-\n",
        "densely populated dangerous 2\n",
        "run 1\n",
        "enable in 1\n",
        "get with 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pollution VS fracking\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "raise with 2\n",
        "pathways from 2\n",
        "associate with 1\n",
        "blame on 1\n",
        "drill 1\n",
        "<-\n",
        "cause 6\n",
        "fail amid about 3\n",
        "determine whether 2\n",
        "link 2\n",
        "actually produce between "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Climate change VS mankind\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "cause on 23\n",
        "particularly fuelsincreased already affect on endanger by 3\n",
        "<-\n",
        "not is contribute 3\n",
        "interface with 3\n",
        "solve 3\n",
        "avoid 2\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS Emissions\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "describe 2\n",
        "recently issue become 2\n",
        "escape because 2\n",
        "even combine with <-\n",
        "certainly fall as involve in 2\n",
        "make unlikely that 2\n",
        "prevent 2\n",
        "contain than 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Drilling VS natural gas\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extract from 5\n",
        "open in 2\n",
        "associate with 1\n",
        "begin as 1\n",
        "create for 1\n",
        "<-\n",
        "boom in 5\n",
        "free from 2\n",
        "help 1\n",
        "reach 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Governance VS regulation\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "force 97\n",
        "drive about 6\n",
        "introduce 6\n",
        "govern 5\n",
        "impose on 5\n",
        "<-\n",
        "introduce by 1\n",
        "guide on 1\n",
        "give out 1\n",
        "propose by 1\n",
        "use 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS Oil\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "surpass as 6\n",
        "increase 2\n",
        "exportsParallel 2\n",
        "equal as 1\n",
        "even surpass as <-\n",
        "pass on 2\n",
        "announce abandon for 2\n",
        "transport 3\n",
        "open up 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Climate VS Climate change\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "anomalies as 6\n",
        "expose alike 5\n",
        "add about 2\n",
        "spend in 2\n",
        "reduce by 2\n",
        "<-\n",
        "not distinguished from be 1\n",
        "Check for 1\n",
        "publish by 1\n",
        "real as 1\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fuel VS natural gas\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "use 2\n",
        "range from 1\n",
        "Asias 1\n",
        "deter 1\n",
        "combine from 1\n",
        "<-\n",
        "acceptable 6\n",
        "bridge 5\n",
        "become for 6\n",
        "replace 5\n",
        "play as 3\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Oil VS Prices\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "develop in 2\n",
        "probably wont though decline in 2\n",
        "create anchor on 1\n",
        "make sure that 1\n",
        "<-\n",
        "drive away from 11\n",
        "suddenly make tie 4\n",
        "displace 2\n",
        "surge in 2\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Climate VS warming\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "anomalies as 6\n",
        "hold 4\n",
        "be indiscernible from 2\n",
        "mitigate 1\n",
        "over-project 1\n",
        "<-\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Coal VS US Environmental Protection Agency\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "see on 2\n",
        "control 1\n",
        "build with 2\n",
        "significantly issued during head 1\n",
        "<-\n",
        "base on 4\n",
        "control 3\n",
        "regulate 4\n",
        "make 3\n",
        "rule 3\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adaptation VS Climate change\n",
        "0.0\n",
        "->"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "base on 3\n",
        "also lead Is 1\n",
        "offset 1\n",
        "inform by 1\n",
        "<-\n",
        "drive within 2\n",
        "release in 2\n",
        "help 1\n",
        "address through 1\n",
        "identify 1\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(result_to_rank, key=lambda x:x[1])[:-3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 96,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_text = \"\"\"The environmental movement has advanced three arguments in recent years for giving up fossil fuels: (1) that we will soon run out of them anyway; (2) that alternative sources of energy will price them out of the marketplace; and (3) that we cannot afford the climate consequences of burning them.\n",
      "\n",
      "These days, not one of the three arguments is looking very healthy. In fact, a more realistic assessment of our energy and environmental situation suggests that, for decades to come, we will continue to rely overwhelmingly on the fossil fuels that have contributed so dramatically to the world\u2019s prosperity and progress.\n",
      "\n",
      "In 2013, about 87% of the energy that the world consumed came from fossil fuels, a figure that\u2014remarkably\u2014was unchanged from 10 years before. This roughly divides into three categories of fuel and three categories of use: oil used mainly for transport, gas used mainly for heating, and coal used mainly for electricity.\n",
      "\n",
      "Over this period, the overall volume of fossil-fuel consumption has increased dramatically, but with an encouraging environmental trend: a diminishing amount of carbon-dioxide emissions per unit of energy produced. The biggest contribution to decarbonizing the energy system has been the switch from high-carbon coal to lower-carbon gas in electricity generation.\n",
      "\n",
      "On a global level, renewable energy sources such as wind and solar have contributed hardly at all to the drop in carbon emissions, and their modest growth has merely made up for a decline in the fortunes of zero-carbon nuclear energy. (The reader should know that I have an indirect interest in coal through the ownership of land in Northern England on which it is mined, but I nonetheless applaud the displacement of coal by gas in recent years.)\n",
      "\n",
      "The argument that fossil fuels will soon run out is dead, at least for a while. The collapse of the price of oil over the past six months is the result of abundance: an inevitable consequence of the high oil prices of recent years, which stimulated innovation in hydraulic fracturing, horizontal drilling, seismology and information technology. The U.S.\u2014the country with the oldest and most developed hydrocarbon fields\u2014has found itself once again, surprisingly, at the top of the energy-producing league, rivaling Saudi Arabia in oil and Russia in gas.\n",
      "\n",
      "The shale genie is now out of the bottle. Even if the current low price drives out some high-cost oil producers\u2014in the North Sea, Canada, Russia, Iran and offshore, as well as in America\u2014shale drillers can step back in whenever the price rebounds. As Mark Hill of Allegro Development Corporation argued last week, the frackers are currently experiencing their own version of Moore\u2019s law: a rapid fall in the cost and time it takes to drill a well, along with a rapid rise in the volume of hydrocarbons they are able to extract.\n",
      "\n",
      "MORE SATURDAY ESSAYS\n",
      "\n",
      "The Coming Chinese Crackup\n",
      "Dave Barry: The Greatest (Party) Generation\n",
      "Putin the Improviser\n",
      "Our Amazingly Plastic Brains\n",
      "The Return of Anti-Semitism\n",
      "Immigration and Islam: Europe\u2019s Crisis of Faith\n",
      "And the shale revolution has yet to go global. When it does, oil and gas in tight rock formations will give the world ample supplies of hydrocarbons for decades, if not centuries. Lurking in the wings for later technological breakthroughs is methane hydrate, a seafloor source of gas that exceeds in quantity all the world\u2019s coal, oil and gas put together.\n",
      "\n",
      "So those who predict the imminent exhaustion of fossil fuels are merely repeating the mistakes of the U.S. presidential commission that opined in 1922 that \u201calready the output of gas has begun to wane. Production of oil cannot long maintain its present rate.\u201d Or President Jimmy Carter when he announced on television in 1977 that \u201cwe could use up all the proven reserves of oil in the entire world by the end of the next decade.\u201d\n",
      "\n",
      "That fossil fuels are finite is a red herring. The Atlantic Ocean is finite, but that does not mean that you risk bumping into France if you row out of a harbor in Maine. The buffalo of the American West were infinite, in the sense that they could breed, yet they came close to extinction. It is an ironic truth that no nonrenewable resource has ever run dry, while renewable resources\u2014whales, cod, forests, passenger pigeons\u2014have frequently done so.\n",
      "\n",
      "The second argument for giving up fossil fuels is that new rivals will shortly price them out of the market. But it is not happening. The great hope has long been nuclear energy, but even if there is a rush to build new nuclear power stations over the next few years, most will simply replace old ones due to close. The world\u2019s nuclear output is down from 6% of world energy consumption in 2003 to 4% today. It is forecast to inch back up to just 6.7% by 2035, according the Energy Information Administration.\n",
      "\n",
      "Nuclear\u2019s problem is cost. In meeting the safety concerns of environmentalists, politicians and regulators added requirements for extra concrete, steel and pipework, and even more for extra lawyers, paperwork and time. The effect was to make nuclear plants into huge and lengthy boondoggles with no competition or experimentation to drive down costs. Nuclear is now able to compete with fossil fuels only when it is subsidized.\n",
      "\n",
      "ENLARGE\n",
      "ILLUSTRATION: HARRY CAMPBELL\n",
      "As for renewable energy, hydroelectric is the biggest and cheapest supplier, but it has the least capacity for expansion. Technologies that tap the energy of waves and tides remain unaffordable and impractical, and most experts think that this won\u2019t change in a hurry. Geothermal is a minor player for now. And bioenergy\u2014that is, wood, ethanol made from corn or sugar cane, or diesel made from palm oil\u2014is proving an ecological disaster: It encourages deforestation and food-price hikes that cause devastation among the world\u2019s poor, and per unit of energy produced, it creates even more carbon dioxide than coal.\n",
      "\n",
      "Wind power, for all the public money spent on its expansion, has inched up to\u2014wait for it\u20141% of world energy consumption in 2013. Solar, for all the hype, has not even managed that: If we round to the nearest whole number, it accounts for 0% of world energy consumption.\n",
      "\n",
      "Both wind and solar are entirely reliant on subsidies for such economic viability as they have. World-wide, the subsidies given to renewable energy currently amount to roughly $10 per gigajoule: These sums are paid by consumers to producers, so they tend to go from the poor to the rich, often to landowners (I am a landowner and can testify that I receive and refuse many offers of risk-free wind and solar subsidies).\n",
      "\n",
      "It is true that some countries subsidize the use of fossil fuels, but they do so at a much lower rate\u2014the world average is about $1.20 per gigajoule\u2014and these are mostly subsidies for consumers (not producers), so they tend to help the poor, for whom energy costs are a disproportionate share of spending.\n",
      "\n",
      "The costs of renewable energy are coming down, especially in the case of solar. But even if solar panels were free, the power they produce would still struggle to compete with fossil fuel\u2014except in some very sunny locations\u2014because of all the capital equipment required to concentrate and deliver the energy. This is to say nothing of the great expanses of land on which solar facilities must be built and the cost of retaining sufficient conventional generator capacity to guarantee supply on a dark, cold, windless evening.\n",
      "\n",
      "The two fundamental problems that renewables face are that they take up too much space and produce too little energy. Consider Solar Impulse, the solar-powered airplane now flying around the world. Despite its huge wingspan (similar to a 747), slow speed and frequent stops, the only cargo that it can carry is the pilots themselves. That is a good metaphor for the limitations of renewables.\n",
      "\n",
      "To run the U.S. economy entirely on wind would require a wind farm the size of Texas, California and New Mexico combined\u2014backed up by gas on windless days. To power it on wood would require a forest covering two-thirds of the U.S., heavily and continually harvested.\n",
      "\n",
      "John Constable, who will head a new Energy Institute at the University of Buckingham in Britain, points out that the trickle of energy that human beings managed to extract from wind, water and wood before the Industrial Revolution placed a great limit on development and progress. The incessant toil of farm laborers generated so little surplus energy in the form of food for men and draft animals that the accumulation of capital, such as machinery, was painfully slow. Even as late as the 18th century, this energy-deprived economy was sufficient to enrich daily life for only a fraction of the population.\n",
      "\n",
      "Our old enemy, the second law of thermodynamics, is the problem here. As a teenager\u2019s bedroom generally illustrates, left to its own devices, everything in the world becomes less ordered, more chaotic, tending toward \u201centropy,\u201d or thermodynamic equilibrium. To reverse this tendency and make something complex, ordered and functional requires work. It requires energy.\n",
      "\n",
      "The more energy you have, the more intricate, powerful and complex you can make a system. Just as human bodies need energy to be ordered and functional, so do societies. In that sense, fossil fuels were a unique advance because they allowed human beings to create extraordinary patterns of order and complexity\u2014machines and buildings\u2014with which to improve their lives.\n",
      "\n",
      "The result of this great boost in energy is what the economic historian and philosopher Deirdre McCloskey calls the Great Enrichment. In the case of the U.S., there has been a roughly 9,000% increase in the value of goods and services available to the average American since 1800, almost all of which are made with, made of, powered by or propelled by fossil fuels.\n",
      "\n",
      "Still, more than a billion people on the planet have yet to get access to electricity and to experience the leap in living standards that abundant energy brings. This is not just an inconvenience for them: Indoor air pollution from wood fires kills four million people a year. The next time that somebody at a rally against fossil fuels lectures you about her concern for the fate of her grandchildren, show her a picture of an African child dying today from inhaling the dense muck of a smoky fire.\n",
      "\n",
      "Notice, too, the ways in which fossil fuels have contributed to preserving the planet. As the American author and fossil-fuels advocate Alex Epstein points out in a bravely unfashionable book, \u201cThe Moral Case for Fossil Fuels,\u201d the use of coal halted and then reversed the deforestation of Europe and North America. The turn to oil halted the slaughter of the world\u2019s whales and seals for their blubber. Fertilizer manufactured with gas halved the amount of land needed to produce a given amount of food, thus feeding a growing population while sparing land for wild nature.\n",
      "\n",
      "To throw away these immense economic, environmental and moral benefits, you would have to have a very good reason. The one most often invoked today is that we are wrecking the planet\u2019s climate. But are we?\n",
      "\n",
      "Although the world has certainly warmed since the 19th century, the rate of warming has been slow and erratic. There has been no increase in the frequency or severity of storms or droughts, no acceleration of sea-level rise. Arctic sea ice has decreased, but Antarctic sea ice has increased. At the same time, scientists are agreed that the extra carbon dioxide in the air has contributed to an improvement in crop yields and a roughly 14% increase in the amount of all types of green vegetation on the planet since 1980.\n",
      "\n",
      "That carbon-dioxide emissions should cause warming is not a new idea. In 1938, the British scientist Guy Callender thought that he could already detect warming as a result of carbon-dioxide emissions. He reckoned, however, that this was \u201clikely to prove beneficial to mankind\u201d by shifting northward the climate where cultivation was possible.\n",
      "\n",
      "Only in the 1970s and 1980s did scientists begin to say that the mild warming expected as a direct result of burning fossil fuels\u2014roughly a degree Celsius per doubling of carbon-dioxide concentrations in the atmosphere\u2014might be greatly amplified by water vapor and result in dangerous warming of two to four degrees a century or more. That \u201cfeedback\u201d assumption of high \u201csensitivity\u201d remains in virtually all of the mathematical models used to this day by the U.N. Intergovernmental Panel on Climate Change, or IPCC.\n",
      "\n",
      "And yet it is increasingly possible that it is wrong. As Patrick Michaels of the libertarian Cato Institute has written, since 2000, 14 peer-reviewed papers, published by 42 authors, many of whom are key contributors to the reports of the IPCC, have concluded that climate sensitivity is low because net feedbacks are modest. They arrive at this conclusion based on observed temperature changes, ocean-heat uptake and the balance between warming and cooling emissions (mainly sulfate aerosols). On average, they find sensitivity to be 40% lower than the models on which the IPCC relies.\n",
      "\n",
      "If these conclusions are right, they would explain the failure of the Earth\u2019s surface to warm nearly as fast as predicted over the past 35 years, a time when\u2014despite carbon-dioxide levels rising faster than expected\u2014the warming rate has never reached even two-tenths of a degree per decade and has slowed down to virtually nothing in the past 15 to 20 years. This is one reason the latest IPCC report did not give a \u201cbest estimate\u201d of sensitivity and why it lowered its estimate of near-term warming.\n",
      "\n",
      "Most climate scientists remain reluctant to abandon the models and take the view that the current \u201chiatus\u201d has merely delayed rapid warming. A turning point to dangerously rapid warming could be around the corner, even though it should have shown up by now. So it would be wise to do something to cut our emissions, so long as that something does not hurt the poor and those struggling to reach a modern standard of living.\n",
      "\n",
      "We should encourage the switch from coal to gas in the generation of electricity, provide incentives for energy efficiency, get nuclear power back on track and keep developing solar power and electricity storage. We should also invest in research on ways to absorb carbon dioxide from the air, by fertilizing the ocean or fixing it through carbon capture and storage. Those measures all make sense. And there is every reason to promote open-ended research to find some unexpected new energy technology.\n",
      "\n",
      "The one thing that will not work is the one thing that the environmental movement insists upon: subsidizing wealthy crony capitalists to build low-density, low-output, capital-intensive, land-hungry renewable energy schemes, while telling the poor to give up the dream of getting richer through fossil fuels.\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "web = {}\n",
      "web['1'] = ['web/www.wsj.com-1426282420',\n",
      "  '2015-03-15 04:42:45.409603+00',\n",
      "  'Fossil Fuels Will Save the World (Really)',\n",
      "  n_text]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}